{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load texts\n",
    "\n",
    "The dataset can be downloaded from http://ai.stanford.edu/~amaas/data/sentiment/.\n",
    "It is binary sentiment classification for the Imdb movie review dataset. This set has 25,000 movie reviews, with 12,500 positive reviews and 12,500 negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "datapath ='aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = os.path.join(datapath, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "path = [os.path.join(train_dir, 'neg')]\n",
    "\n",
    "for l in ['neg', 'pos']:\n",
    "    path = os.path.join(train_dir, l)\n",
    "    \n",
    "    if l== 'neg':\n",
    "        label = [0,1]\n",
    "    else :\n",
    "        label = [1,0]\n",
    "            \n",
    "    for fname in os.listdir(path):\n",
    "        fpath = os.path.join(path, fname)\n",
    "        f = open(fpath)\n",
    "        t = cleanSentences( f.read())\n",
    "        texts.append(t)\n",
    "        \n",
    "        labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i remember watching this movie several times as a very young kid and there were parts of it many in fact that i did not understand i think i have seen it once as an adult and i then understood those parts the only problem with viewing it as an adult was that it was not entertaining to me at all so what kind of movie is this is it a kids movie not hardly it contains language and subject matter not suitable for kids is it a hyperbole of what every parent feels like they are going through with their own children maybe but then why wouldnt it focus more on john ritters character instead of junior when a film has a 7yearold as its main character in order to do well with its audience it should be a movie for the seven and under crowd otherwise people older than that will have no way to relate even 8yearolds wouldnt want to see a movie about a kid who is whole year younger than them im pretty sure this film did not do well in the box office and the reason has to be because it was unable to find a niche in the market'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample of the data\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 111525 unique tokens.\n",
      "Shape of data tensor: (25000, 250)\n",
      "Shape of label tensor: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxSeqLength)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading pretrained word vectors from Glove\n",
    "\n",
    "We use a smaller and more manageable matrix from GloVe, which contain 400,000 word vectors, each with a dimensionality of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(filename)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxSeqLength,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (With LSTM units) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, \\\n",
    "    Activation, concatenate, GRU, Embedding, Flatten, LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(maxSeqLength,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = LSTM(64)(embedded_sequences)\n",
    "x = Dropout(0.25)(x)\n",
    "output = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(sequence_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=0.001)\n",
    "model.compile( loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.6117 - acc: 0.6605 - val_loss: 0.5574 - val_acc: 0.7362\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.5374 - acc: 0.7396 - val_loss: 0.5674 - val_acc: 0.7172\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.5818 - acc: 0.6904 - val_loss: 0.4847 - val_acc: 0.7696\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.4801 - acc: 0.7767 - val_loss: 0.4587 - val_acc: 0.7938\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.4486 - acc: 0.7928 - val_loss: 0.4253 - val_acc: 0.8094\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.4232 - acc: 0.8064 - val_loss: 0.4042 - val_acc: 0.8168\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 53s - loss: 0.4011 - acc: 0.8198 - val_loss: 0.3967 - val_acc: 0.8258\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 53s - loss: 0.3745 - acc: 0.8362 - val_loss: 0.3598 - val_acc: 0.8426\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 53s - loss: 0.3697 - acc: 0.8365 - val_loss: 0.3782 - val_acc: 0.8346\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.3460 - acc: 0.8516 - val_loss: 0.3642 - val_acc: 0.8388\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.3282 - acc: 0.8587 - val_loss: 0.3553 - val_acc: 0.8488\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.3244 - acc: 0.8623 - val_loss: 0.3435 - val_acc: 0.8528\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 53s - loss: 0.3117 - acc: 0.8673 - val_loss: 0.3428 - val_acc: 0.8550\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 52s - loss: 0.3098 - acc: 0.8683 - val_loss: 0.3433 - val_acc: 0.8530\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 51s - loss: 0.2945 - acc: 0.8772 - val_loss: 0.3381 - val_acc: 0.8506\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 51s - loss: 0.2932 - acc: 0.8756 - val_loss: 0.3268 - val_acc: 0.8648\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 51s - loss: 0.2774 - acc: 0.8848 - val_loss: 0.3720 - val_acc: 0.8400\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 51s - loss: 0.2818 - acc: 0.8811 - val_loss: 0.3341 - val_acc: 0.8582\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 51s - loss: 0.2626 - acc: 0.8923 - val_loss: 0.3369 - val_acc: 0.8666\n",
      "Epoch 20/20\n",
      "12544/20000 [=================>............] - ETA: 17s - loss: 0.2542 - acc: 0.8933"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
